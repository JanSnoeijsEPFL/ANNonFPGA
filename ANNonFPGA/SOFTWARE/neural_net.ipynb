{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import randint\n",
    "#import framework\n",
    "#import implementations as func\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import scipy.signal as scp\n",
    "import tensorflow as tf\n",
    "import target_data_gen\n",
    "from target_data_gen import get_sizes\n",
    "from target_data_gen import target_gen\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear():  \n",
    "  \n",
    "    # initialization function\n",
    "    def __init__(self, inputs, outputs):\n",
    "        print(\"creating Linear layer\")\n",
    "        lim = np.sqrt(6/(inputs+outputs))\n",
    "        self.W = np.random.uniform(-lim,lim,(inputs+1, outputs))\n",
    "        self.W[0,:].fill(0)\n",
    "        self.dW = np.zeros((inputs+1, outputs))\n",
    "        self.Linear_updateW = Optimizer(self.W)\n",
    "    # forward pass calculation\n",
    "    # receives input tensor of shape : [nb_samples x insize]\n",
    "    def forward(self,X): \n",
    "        # [nb_samples x insize] x [insize x outsize] + [nb_samples x outsize] => [nb_samples x outsize]\n",
    "        H = X @ self.W\n",
    "        return H\n",
    "    \n",
    "    # backward pass calculation\n",
    "    # receives gradient of the next layer\n",
    "    # returns gradient with respect to the input. \n",
    "    # returns derivative with respect to the weights.\n",
    "    def backward(self,dH, X):\n",
    "        # matrix multiplication [nb_samples, outsize]*[insize, outsize]^t => [nb_samples x insize]\n",
    "        # sum over samples for parameters (normal Gradient Descent. If divided into batches -> SGD)\n",
    "        dX = dH @ np.transpose(self.W)\n",
    "        #print(dH, self.dW)\n",
    "        self.dW = np.transpose(X) @ dH\n",
    "        self.W = self.Linear_updateW.adam_update(self.W, self.dW)\n",
    "        return dX, self.dW\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU():\n",
    "    #Wz, Wr, Wh, Uz, Ur, Uh = np.array()\n",
    "    #z, r, h, s = np.array()\n",
    "    \n",
    "    def __init__(self,sequences, timesteps, inputs, outputs):\n",
    "        print(\"creating GRU layer\")\n",
    "        lim_w = np.sqrt(6/(inputs+outputs))\n",
    "        lim_u = np.sqrt(6/(2*outputs))\n",
    "        self.Wz, self.Wr, self.Wh = np.random.uniform(-lim_w,lim_w,(inputs+1, outputs)), np.random.uniform(-lim_w,lim_w,(inputs+1, outputs)), np.random.uniform(-lim_w,lim_w,(inputs+1, outputs))\n",
    "        print(self.Wz.shape)\n",
    "        self.Wr[0,:].fill(-1)\n",
    "        self.Wz[0,:].fill(0)\n",
    "        self.Wh[0,:].fill(0)\n",
    "        self.Uz, self.Ur, self.Uh = np.random.uniform(-lim_u,lim_u,(outputs, outputs)), np.random.uniform(-lim_u,lim_u,(outputs, outputs)), np.random.uniform(-lim_u,lim_u,(outputs, outputs))\n",
    "        self.z, self.r = np.zeros((sequences,timesteps,outputs)),np.zeros((sequences,timesteps,outputs))\n",
    "        self.h, self.s = np.zeros((sequences,timesteps,outputs)),np.zeros((sequences,timesteps,outputs))\n",
    "        print(self.s.shape)\n",
    "        self.dWz, self.dWr, self.dWh = np.zeros((inputs+1, outputs)), np.zeros((inputs+1, outputs)), np.zeros((inputs+1, outputs))\n",
    "        self.dUz, self.dUr, self.dUh = np.zeros((outputs, outputs)), np.zeros((outputs, outputs)), np.zeros((outputs, outputs))\n",
    "        self.GRU_updateWz = Optimizer(self.Wz)\n",
    "        self.GRU_updateWr = Optimizer(self.Wr)\n",
    "        self.GRU_updateWh = Optimizer(self.Wh)\n",
    "        self.GRU_updateUz = Optimizer(self.Uz)\n",
    "        self.GRU_updateUr = Optimizer(self.Ur)\n",
    "        self.GRU_updateUh = Optimizer(self.Uh)\n",
    "    def forward(self,X):\n",
    "        # initialize\n",
    "        # first iteration\n",
    "        #print(self.z.shape, X.shape)\n",
    "        self.z[:,0,:] = sigmoid(X[:,0,:] @ self.Wz) #[seq * time * OUT] = [seq * time * IN] @ [IN * OUT]\n",
    "        self.r[:,0,:] = sigmoid(X[:,0,:] @ self.Wr)\n",
    "        self.h[:,0,:] = tanh(X[:,0,:] @ self.Wh)\n",
    "        self.s[:,0,:] = self.z[:,0,:]*self.h[:,0,:]\n",
    "        for t in range(1, X.shape[1]):\n",
    "            self.z[:,t,:] = sigmoid(X[:,t,:] @ self.Wz + self.s[:,t-1,:] @ self.Uz)\n",
    "            self.r[:,t,:] = sigmoid(X[:,t,:] @ self.Wr + self.s[:,t-1,:] @ self.Ur)\n",
    "            self.h[:,t,:] = tanh(X[:,t,:] @ self.Wh + self.r[:,t,:] * (self.s[:,t-1,:] @ self.Uh))\n",
    "            self.s[:,t,:] = self.z[:,t,:]*self.h[:,t,:] + (1-self.z[:,t,:])*self.s[:,t-1,:]\n",
    "        return self.s\n",
    "    \n",
    "    def backward(self, ds, X):\n",
    "        dsnext = np.zeros_like(self.s[:,0,:])\n",
    "        dX = np.zeros((X.shape))\n",
    "        for t in reversed(range(X.shape[1])):\n",
    "            #print(ds.shape, self.z.shape)\n",
    "            dh = ds*self.z[:,t,:]\n",
    "            dh_l = dh*tanh(self.h[:,t,:], deriv=True)\n",
    "            #print(\"GRU backward\", dh_l.shape)\n",
    "            self.dWh += np.transpose(X[:,t,:]) @ dh_l # [1 x IN].T @ [1 x OUT]\n",
    "           \n",
    "            #print(self.dWh.shape)\n",
    "            self.dUh += np.transpose(self.r[:,t,:]*self.s[:,t-1,:]) @ dh_l # ([1 x OUT] * [1 x OUTin]).T @ [1 x OUT]\n",
    "            \n",
    "            #drsp = dh_l @ np.transpose(Uh)# [OUTin x OUT] @ [1 x OUT]\n",
    "            drsp = dh_l @ np.transpose(self.Uh)\n",
    "            dr = dh_l * (self.s[:,t-1,:] @ self.Uh) #[1 x OUT] = [ 1 x OUT]*([1 x OUTin] @ [OUTin x OUT])\n",
    "            dr_l = dr * sigmoid(self.r[:,t,:], deriv=True)\n",
    "            \n",
    "            self.dWr += np.transpose(X[:,t,:]) @ dr_l # [ IN x OUT] = [1 x IN].T @ [1 x OUT]\n",
    "            self.dUr += np.transpose(self.s[:,t-1,:]) @ dr_l # [OUTin x OUT ] = [ 1 x OUTin].T @ [ 1 x OUT]\n",
    "            \n",
    "            dz = (self.h[:,t,:] - self.s[:,t-1,:]) * dh  # [1 x OUT] = ( [1 x OUT] - [1 x OUTin] ) * [1 x OUT]\n",
    "            dz_l = dz * sigmoid(self.z[:,t,:], deriv=True)\n",
    "            \n",
    "            self.dWz += np.transpose(X[:,t,:]) @ dz_l\n",
    "            self.dUz += np.transpose(self.s[:,t-1,:]) @ dz_l\n",
    "            \n",
    "            # calculate gradient w.r.t s[t-1]\n",
    "            ds_fz_inner = dz_l @ np.transpose(self.Uh) #  [1 x OUTin] =  [1 x OUT] @ [OUTin x OUT].T\n",
    "            ds_fz = ds * (1-self.z[:,t,:]) # [1 x OUTin] = [1 x OUT] * [1 x OUT]\n",
    "            ds_fh = drsp * self.r[:,t,:] # [1 x OUTin] = [1 x OUT] * [1 x OUT]\n",
    "            ds_fr = dr_l @ np.transpose(self.Ur)\n",
    "            \n",
    "            dsnext = ds_fz_inner + ds_fz + ds_fh + ds_fr\n",
    "            ds += dsnext\n",
    "            dX[:,t,:]=dh_l @ np.transpose(self.Wh) + dr_l @ np.transpose(self.Wr) + dz_l @ np.transpose(self.Wz)\n",
    "            \n",
    "            #update weights\n",
    "        self.Wz = self.GRU_updateWz.adam_update(self.Wz, self.dWz)\n",
    "        self.Wr = self.GRU_updateWr.adam_update(self.Wr, self.dWr)\n",
    "        self.Wh = self.GRU_updateWh.adam_update(self.Wh, self.dWh)\n",
    "        self.Uz = self.GRU_updateUz.adam_update(self.Uz, self.dUz)\n",
    "        self.Ur = self.GRU_updateUr.adam_update(self.Ur, self.dUr)\n",
    "        self.Uh = self.GRU_updateUh.adam_update(self.Uh, self.dUh) \n",
    "        return ds, dX\n",
    "    #def compute_gradients(self,X):\n",
    "        \n",
    "    def change_input_size(self, sequences, timesteps,outputs):\n",
    "        self.z, self.r = np.zeros((sequences,timesteps,outputs)),np.zeros((sequences,timesteps,outputs))\n",
    "        self.h, self.s = np.zeros((sequences,timesteps,outputs)),np.zeros((sequences,timesteps,outputs))\n",
    "        \n",
    "    def get_parameters(self):\n",
    "        return self.Wz, self.dWz,self.Wr, self.dWr,self.Wh, self.dWh,self.Uz, self.dUz,self.Ur, self.dUr,self.Uh, self.dUh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3 4 5 6 7 8 9]\n",
      " [1 2 3 4 5 6 7 8 9]]\n",
      "creating GRU layer\n",
      "(9, 5)\n",
      "(1, 2, 5) (1, 2, 9)\n",
      "[[[0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([np.arange(1,10), np.arange(1,10)])\n",
    "X.reshape(1,2,9)\n",
    "print(X)\n",
    "GRU_layer_0 = GRU(sequences=1, timesteps=2, inputs=9, outputs=5)\n",
    "s = GRU_layer_0.forward(X.reshape(1,2,9))\n",
    "\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(input, deriv=False):\n",
    "    if deriv:\n",
    "        return input*(1-input)\n",
    "    else:\n",
    "        return 1 / (1 + np.exp(-input))\n",
    "\n",
    "def tanh(input, deriv=False):\n",
    "    if deriv:\n",
    "        return 1 - input ** 2\n",
    "    else:\n",
    "        return np.tanh(input)\n",
    "\n",
    "def reLU(input, deriv=False):\n",
    "    if deriv:\n",
    "        output = np.copy(input)\n",
    "        output[input > 0.0 ] = 1.0\n",
    "        output[input <= 0.0] = 0.0\n",
    "        return output\n",
    "    else:\n",
    "        output = np.copy(input)\n",
    "        output[input < 0.0 ] = 0.0\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 10, 2)\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "input_shape = (32,10,2)\n",
    "X=np.zeros((input_shape))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D():\n",
    "    height = 0\n",
    "    width = 0\n",
    "    nb_seq = 0\n",
    "    timesteps = 0\n",
    "    new_height = 0\n",
    "    new_width = 0\n",
    "    K = 0\n",
    "    M = 0\n",
    "    N = 0\n",
    "    def __init__(self, kernel_height, kernel_width, filters):\n",
    "        self.W = np.random.uniform(-1,1,(kernel_height, kernel_width, filters))\n",
    "        self.B = np.zeros((1, filters))\n",
    "        self.dW = np.zeros((kernel_height, kernel_width, filters))\n",
    "        self.dB = np.zeros((1, filters))\n",
    "        print('Creating Conv2D layer')\n",
    "        self.Conv2D_updateW = Optimizer(self.W)\n",
    "        self.Conv2D_updateB = Optimizer(self.B)\n",
    "    # W.shape : [kernel_height, kernel_width, nb_filters]\n",
    "    # X.shape : [samples, timesteps, height, width, 1]\n",
    "    # B.shape : [1, nb_filters]\n",
    "    def forward(self, X):\n",
    "        print(\"conv2d Xshape\", X.shape)\n",
    "        self.height = X.shape[2]\n",
    "        self.width = X.shape[3]\n",
    "        self.M = self.W.shape[0]\n",
    "        self.N = self.W.shape[1]\n",
    "        self.K = self.W.shape[2]\n",
    "        self.nb_seq = X.shape[0]\n",
    "        self.timesteps = X.shape[1]\n",
    "        #print(self.timesteps)\n",
    "        #compute new dimensions\n",
    "        self.new_height = self.height - self.M + 1\n",
    "        self.new_width = self.width - self.N + 1\n",
    "        \n",
    "       # print(new_height, new_width)\n",
    "        h = np.zeros((self.nb_seq, self.timesteps, self.new_height, self.new_width, self.K))\n",
    "        for k in range(self.K):\n",
    "            for i in range(self.new_height):\n",
    "                for j in range(self.new_width):\n",
    "                    h[:,:,i,j,k]=np.sum(X[:,:,i:i+self.M, j:j+self.N,0]*self.W[:,:,k], axis =(2,3))+self.B[0,k]\n",
    "        return h\n",
    "    # dH has dimensions of H which means in case of X.shape=[3,3,1], W.shape=[2,2,1] => H.shape=[2,2,1]\n",
    "    # dX should have the same shape as X, i.e dX.shape=[3,3,1] = dH conv2D flipped W 'FULL'\n",
    "    def backward(self, dH, X):\n",
    "        # dw is the same operation as in forward propagation. \n",
    "        # no need to compute dX in our case because the conv2D layer is the first layer and means the end of backprop. algorithm.\n",
    "        for k in range(self.K):\n",
    "            self.dB[:,k] = np.sum(dH[:,:,:,:,k])\n",
    "            for i in range(self.M):\n",
    "                for j in range(self.N):\n",
    "                    #average over all the sequences and timesteps\n",
    "                    self.dW[i,j,k]=np.sum(X[:,:,i:i+self.new_height, j:j+self.new_width,0]*dH[:,:,:,:,k])\n",
    "        self.W = self.Conv2D_updateW.adam_update(self.W, self.dW)\n",
    "        self.B = self.Conv2D_updateB.adam_update(self.B, self.dB)\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 1)\n",
      "(3, 3, 1)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 3 required positional arguments: 'kernel_height', 'kernel_width', and 'filters'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-14480a324581>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mB\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mconv_layer1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv_layer1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 3 required positional arguments: 'kernel_height', 'kernel_width', and 'filters'"
     ]
    }
   ],
   "source": [
    "#test for conv2d\n",
    "X=np.array([[3.0,5,6], [1,2,4], [2,3,5]]).reshape(3,3,1)\n",
    "W=np.array([[2,1.0],[1,4]]).reshape(-1,2,1)\n",
    "#W = np.rot90(W,2)\n",
    "print(W.shape)\n",
    "B=np.array([0.5]).reshape(-1,1)\n",
    "print(X.shape)\n",
    "conv_layer1 = Conv2D()\n",
    "H = conv_layer1.forward(X, W, B)\n",
    "print(H, H.shape)\n",
    "print(H[1,0,0])\n",
    "\n",
    "# check against scp : kernel is flipped here.\n",
    "X_scp = X.reshape(-1,3)\n",
    "W_scp = W.reshape(-1,2)\n",
    "H_scp = scp.convolve2d(X_scp, W_scp, mode = 'valid')\n",
    "#print(H_scp)\n",
    "#print(X[1,0,0])\n",
    "\n",
    "#check against tensorflow\n",
    "\n",
    "X_tf = X.reshape(1,3,3,1)\n",
    "W_tf = W.reshape(2,2,1,1)\n",
    "sess = tf.Session()\n",
    "H_tf = sess.run(tf.nn.conv2d(X_tf, W_tf, strides = (1,1,1,1), padding = 'VALID'))\n",
    "sess.run(tf.Print(H_tf, [H_tf]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool2D():\n",
    "    height = 0\n",
    "    width = 0\n",
    "    M = 0\n",
    "    N = 0\n",
    "    K = 0\n",
    "    nb_seq = 0\n",
    "    timesteps = 0\n",
    "    def __init__(self):\n",
    "        print('creating 2D Max pooling layer')\n",
    "    \n",
    "    def forward(self, X, pool):\n",
    "        self.height = X.shape[2]\n",
    "        self.width = X.shape[3]\n",
    "        self.M = pool[0]\n",
    "        self.N = pool[1]\n",
    "        self.K = X.shape[4]\n",
    "        self.nb_seq = X.shape[0]\n",
    "        self.nb_timesteps = X.shape[1]\n",
    "        if self.height%2 == 1:\n",
    "            X = np.delete(X, obj=self.height-1, axis=2)\n",
    "            self.height = X.shape[2]\n",
    "        if self.width%2 == 1:\n",
    "            X = np.delete(X, obj=self.width-1, axis=3)\n",
    "            self.width = X.shape[3]\n",
    "        X_argmax = np.copy(X)\n",
    "       \n",
    "        #print(self.M, self.N)\n",
    "        #compute new sizes\n",
    "        new_height = int(self.height/self.M)\n",
    "        new_width = int(self.width/self.N)\n",
    "        H = np.zeros((self.nb_seq, self.nb_timesteps, new_height, new_width, self.K))\n",
    "        #print(H.shape)\n",
    "       \n",
    "        #start pooling\n",
    "        for k in range(self.K):\n",
    "            for i in range(0,self.height,self.M):\n",
    "                for j in range(0,self.width,self.N): #genericity loss here, only valid with kernel width = 2.\n",
    "                    X_temp = X[:,:,i:i+self.M, j:j+self.N, k].reshape(self.nb_seq, self.nb_timesteps, self.M*self.N,1)\n",
    "                    H[:,:,int(i/self.M), int(j/self.N), k] = np.amax(X[:,:,i:i+self.M, j:j+self.N, k], axis=(2,3)) \n",
    "                    X_argmax[:,:, i, j, k] = (np.argmax(X_temp, axis = 2)).reshape(self.nb_seq, self.nb_timesteps) \n",
    "                    X_argmax[:,:, i+self.M-1, j, k] = (np.argmax(X_temp, axis = 2)).reshape(self.nb_seq, self.nb_timesteps) \n",
    "                    X_argmax[:,:, i, j+self.N-1, k] = (np.argmax(X_temp, axis = 2)).reshape(self.nb_seq, self.nb_timesteps) \n",
    "                    X_argmax[:,:, i+self.M-1, j+self.N-1, k] = (np.argmax(X_temp, axis = 2)).reshape(self.nb_seq, self.nb_timesteps) \n",
    "        return H, X_argmax\n",
    "        \n",
    "    def backward(self, X_argmax, dH):\n",
    "        dX = np.zeros((X_argmax.shape))\n",
    "        for k in range(self.K):\n",
    "            for i in range(0, self.height, self.M):\n",
    "                for j in range(0,self.width, self.N):\n",
    "                    dX[:,:, i:i+self.M,j:j+self.N,k] = self.norm_argmax(X_argmax[:,:,i:i+self.M, j:j+self.N, k], dH[:,:,int(i/self.M), int(j/self.N), k])       \n",
    "        return dX\n",
    "                                                                \n",
    "    def norm_argmax(self, X, dH):\n",
    "        #print(\"X_argmax shape maxpool = \", X.shape)\n",
    "        h = X.shape[2]\n",
    "        w = X.shape[3]\n",
    "        I = np.array(np.arange(X.shape[2]+X.shape[3]))\n",
    "        Ibig = np.zeros_like(X)\n",
    "        #print(\"Maxpoolbackward:\", X.shape, Ibig.shape, I.shape)\n",
    "        Ibig = np.tile(I,(X.shape[0], X.shape[1],1))\n",
    "        #print(\"ibig\", Ibig.shape)\n",
    "        Xresh = X.reshape(X.shape[0], X.shape[1], Ibig.shape[2]) # multidimensional\n",
    "        diff = (Xresh-Ibig).astype(int)\n",
    "        #duplicate dH x 4\n",
    "        dH_dupl = np.zeros_like(X)\n",
    "        for m in range(2):\n",
    "            for n in range(2):\n",
    "                dH_dupl[:,:,m,n] = dH\n",
    "        # reshape to make boolean assignment\n",
    "        Xresh = Xresh.flatten()\n",
    "        diff = diff.flatten()\n",
    "        dHflat = dH_dupl.flatten()\n",
    "        #print(\"dH shape\", dH.shape)\n",
    "        Xresh[diff != 0] = -1\n",
    "        #print(\"xresh shape\", Xresh.shape, diff.shape)\n",
    "        deriv = np.copy(Xresh)\n",
    "        deriv[Xresh > -1.0] = dHflat[Xresh > -1.0]\n",
    "        deriv[Xresh == -1.0] = 0.0\n",
    "        deriv = deriv.reshape(X.shape[0],X.shape[1],h, w)\n",
    "        #print(\"deriv shape max pool = \", deriv.shape)\n",
    "        return deriv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1 2]\n",
      "  [3 4]]\n",
      "\n",
      " [[5 6]\n",
      "  [7 8]]] (2, 2, 2)\n",
      "3\n",
      "[[1 2]\n",
      " [3 4]] (2, 2)\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test \n",
    "A = np.array([[[1,2],[3,4]],[[5,6],[7,8]]])\n",
    "print(A, A.shape)\n",
    "print(A[0,1,0])\n",
    "B=np.array([[1,2],[3,4]])\n",
    "print(B, B.shape)\n",
    "print(B[1,0])\n",
    "int(98/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-caa07f00056f>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-7-caa07f00056f>\"\u001b[1;36m, line \u001b[1;32m11\u001b[0m\n\u001b[1;33m    dXmaxpool_test = max, X_arg[1Pool2D_layer2.backward(X_arg, ddH)\u001b[0m\n\u001b[1;37m                                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# test Max Pool 2D\n",
    "X = np.zeros((1,1,4,4,1))\n",
    "for i in range(X.shape[2]):\n",
    "    for j in range(X.shape[3]):\n",
    "        X[:,:,i,j,:]= i -2*j \n",
    "x_dummy=X.reshape(4,4)\n",
    "ddH = np.ones((5,5,1))\n",
    "maxPool2D_layer2 = MaxPool2D()\n",
    "pool = np.array([2,2])\n",
    "H, X_arg =maxPool2D_layer2.forward(X, pool)\n",
    "dXmaxpool_test = max, X_arg[1Pool2D_layer2.backward(X_arg, ddH)\n",
    "print(X_arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.  4.  6.  8. 10.]\n",
      " [ 4.  6.  8. 10. 12.]\n",
      " [ 6.  8. 10. 12. 14.]\n",
      " [ 8. 10. 12. 14. 16.]\n",
      " [10. 12. 14. 16. 18.]]\n"
     ]
    }
   ],
   "source": [
    "print(H.reshape(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9.]\n",
      " [ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]\n",
      " [ 2.  3.  4.  5.  6.  7.  8.  9. 10. 11.]\n",
      " [ 3.  4.  5.  6.  7.  8.  9. 10. 11. 12.]\n",
      " [ 4.  5.  6.  7.  8.  9. 10. 11. 12. 13.]\n",
      " [ 5.  6.  7.  8.  9. 10. 11. 12. 13. 14.]\n",
      " [ 6.  7.  8.  9. 10. 11. 12. 13. 14. 15.]\n",
      " [ 7.  8.  9. 10. 11. 12. 13. 14. 15. 16.]\n",
      " [ 8.  9. 10. 11. 12. 13. 14. 15. 16. 17.]\n",
      " [ 9. 10. 11. 12. 13. 14. 15. 16. 17. 18.]]\n"
     ]
    }
   ],
   "source": [
    "print(X.reshape(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 1. 0. 1. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 1. 0. 1. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 1. 0. 1. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 1. 0. 1. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 1. 0. 1. 0. 1. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(X_arg.reshape(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function\n",
    "def CrossEntropy(yHat, y):\n",
    "    length = yHat.shape[0]\n",
    "    y = y.flatten()\n",
    "    yHat = yHat.flatten()\n",
    "    print(yHat.shape)\n",
    "    output = np.copy(y)\n",
    "    print(output.shape)\n",
    "    output[y==1] = -10*np.log(yHat)[y==1]\n",
    "    output[y==0] = -np.log(1 - yHat)[y==0]\n",
    "    output = np.sum(output)/y.shape[0]\n",
    "    return output\n",
    "def derivCrossEntropy(yHat, y):\n",
    "    dloss = np.zeros((yHat.shape[0]))\n",
    "    for i in range(yHat.shape[0]):\n",
    "        if yHat[i] < 1 and yHat[i] > 0:\n",
    "            dloss[i] = -y[i]/yHat[i] + (1-y[i])/(1-yHat[i])\n",
    "        elif yHat[i] == 1 and y[i] == 1:\n",
    "            dloss[i] = -1\n",
    "        elif yHat[i] == 0 and y[i] == 0:\n",
    "            dloss[i] = 1\n",
    "        elif yHat[i] == 1 and y[i] == 0:\n",
    "            dloss[i] = 2*63\n",
    "        elif yHat[i] == 0 and y[i] == 1:\n",
    "            dloss[i] = -2*63\n",
    "    return dloss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TB done:\n",
    "# write main function calling forward & backward\n",
    "# write flattening (can be done in MAIN)\n",
    "# try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weight update\n",
    "class Optimizer():\n",
    "    # initialize training parameters\n",
    "    def __init__(self, W):\n",
    "       # self.Wconv = np.zeros((nb_Wconv1, nb_Wconv2))\n",
    "       # self.Bconv = np.zeros((nb_Bconv))\n",
    "       # self.Wr, self.Wh, self.Wz = np.zeros((nb_GRUin, nb_GRUout)), np.zeros((nb_GRUin, nb_GRUout)), np.zeros((nb_GRUin, nb_GRUout))\n",
    "       # self.Ur, self.Uh, self.Uz = np.zeros((nb_GRUout, nb_GRUout)), np.zeros((nb_GRUout, nb_GRUout)), np.zeros((nb_GRUout, nb_GRUout))\n",
    "       # self.Wlin = np.zeros((nb_Wlin)) # only 1 output\n",
    "        self.alpha = 0.01\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.99\n",
    "        self.eps = 10**-8\n",
    "        self.v = np.zeros((W.shape))\n",
    "        self.m = np.zeros((W.shape))\n",
    "    def adam_update(self, W, dW):\n",
    "        np.clip(dW, -5, 5, out=dW)\n",
    "        self.m = self.beta1*self.m + (1 - self.beta1)*dW\n",
    "        self.v = self.beta2*self.v + (1 - self.beta2)*np.power(dW, 2)\n",
    "        m_corr = self.m/(1-self.beta1)\n",
    "        v_corr = self.v/(1-self.beta2)\n",
    "        W = W  - self.alpha*m_corr/(np.sqrt(v_corr)+self.eps)   \n",
    "        return W\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIZE sigbufs (23, 921600)\n",
      "921600\n",
      "(18, 1, 1, 23, 921600, 1)\n",
      "(18, 10000, 1)\n",
      "initial time: 0\n",
      "initial time: 0\n",
      "initial time: 700000\n",
      "initial time: 300000\n",
      "initial time: 0\n",
      "initial time: 0\n",
      "initial time: 0\n",
      "initial time: 0\n",
      "initial time: 0\n",
      "initial time: 0\n",
      "initial time: 0\n",
      "initial time: 0\n",
      "initial time: 0\n",
      "initial time: 0\n",
      "initial time: 400000\n",
      "initial time: 200000\n",
      "(1600, 10, 23, 100, 1) (1600, 1)\n",
      "Creating Conv2D layer\n",
      "creating 2D Max pooling layer\n",
      "creating GRU layer\n",
      "(1079, 100)\n",
      "(1600, 10, 100)\n",
      "creating Linear layer\n",
      "conv2d Xshape (1600, 10, 23, 100, 1)\n",
      "(1600,)\n",
      "(1600,)\n",
      "0.8898600047095209\n",
      "0 (1600, 1) (1600, 1)\n",
      "conv2d Xshape (1600, 10, 23, 100, 1)\n",
      "(1600,)\n",
      "(1600,)\n",
      "1.15810805437319\n",
      "0 (1600, 1) (1600, 1)\n",
      "conv2d Xshape (1600, 10, 23, 100, 1)\n",
      "(1600,)\n",
      "(1600,)\n",
      "0.8756739872830286\n",
      "0 (1600, 1) (1600, 1)\n",
      "conv2d Xshape (1600, 10, 23, 100, 1)\n",
      "(1600,)\n",
      "(1600,)\n",
      "0.7336825699661891\n",
      "0 (1600, 1) (1600, 1)\n",
      "conv2d Xshape (1600, 10, 23, 100, 1)\n",
      "(1600,)\n",
      "(1600,)\n",
      "0.6632243621757129\n",
      "0 (1600, 1) (1600, 1)\n",
      "conv2d Xshape (1600, 10, 23, 100, 1)\n",
      "(1600,)\n",
      "(1600,)\n",
      "0.561406268513523\n",
      "0 (1600, 1) (1600, 1)\n",
      "conv2d Xshape (1600, 10, 23, 100, 1)\n",
      "(1600,)\n",
      "(1600,)\n",
      "0.4557073224108933\n",
      "0 (1600, 1) (1600, 1)\n",
      "conv2d Xshape (1600, 10, 23, 100, 1)\n",
      "(1600,)\n",
      "(1600,)\n",
      "0.5112897491349584\n",
      "0 (1600, 1) (1600, 1)\n",
      "conv2d Xshape (1600, 10, 23, 100, 1)\n",
      "(1600,)\n",
      "(1600,)\n",
      "0.7475232453177634\n",
      "0 (1600, 1) (1600, 1)\n",
      "conv2d Xshape (1600, 10, 23, 100, 1)\n",
      "(1600,)\n",
      "(1600,)\n",
      "0.8645330305971395\n",
      "0 (1600, 1) (1600, 1)\n",
      "conv2d Xshape (1600, 10, 23, 100, 1)\n",
      "(1600,)\n",
      "(1600,)\n",
      "0.8329592374124353\n",
      "0 (1600, 1) (1600, 1)\n",
      "conv2d Xshape (1600, 10, 23, 100, 1)\n",
      "(1600,)\n",
      "(1600,)\n",
      "0.7601704264880726\n",
      "0 (1600, 1) (1600, 1)\n",
      "conv2d Xshape (1600, 10, 23, 100, 1)\n",
      "(1600,)\n",
      "(1600,)\n",
      "0.715585323428892\n",
      "0 (1600, 1) (1600, 1)\n",
      "conv2d Xshape (1600, 10, 23, 100, 1)\n",
      "(1600,)\n",
      "(1600,)\n",
      "0.7277937230971119\n",
      "0 (1600, 1) (1600, 1)\n",
      "conv2d Xshape (1600, 10, 23, 100, 1)\n",
      "(1600,)\n",
      "(1600,)\n",
      "0.7539231894764632\n",
      "0 (1600, 1) (1600, 1)\n",
      "conv2d Xshape (1600, 10, 23, 100, 1)\n",
      "(1600,)\n",
      "(1600,)\n",
      "0.7656925299181242\n",
      "0 (1600, 1) (1600, 1)\n",
      "conv2d Xshape (1600, 10, 23, 100, 1)\n",
      "(1600,)\n",
      "(1600,)\n",
      "0.7609863830327848\n",
      "0 (1600, 1) (1600, 1)\n",
      "conv2d Xshape (1600, 10, 23, 100, 1)\n",
      "(1600,)\n",
      "(1600,)\n",
      "0.7455443838396352\n",
      "0 (1600, 1) (1600, 1)\n",
      "conv2d Xshape (1600, 10, 23, 100, 1)\n",
      "(1600,)\n",
      "(1600,)\n",
      "0.7266281027508282\n",
      "0 (1600, 1) (1600, 1)\n",
      "conv2d Xshape (1600, 10, 23, 100, 1)\n",
      "(1600,)\n",
      "(1600,)\n",
      "0.7130434625721368\n",
      "0 (1600, 1) (1600, 1)\n"
     ]
    }
   ],
   "source": [
    "#main\n",
    "\n",
    "# get data\n",
    "np.random.seed(0)\n",
    "output = 1\n",
    "#X=np.array(sigbufs\n",
    "batch_size = 100 #(length)\n",
    "batch_mod = 10000\n",
    "dataset_size = 18\n",
    "X, Y = list(), list()\n",
    "\n",
    "#**********************CALLLING DATA GENERATOR FUNCTION ***********************\n",
    "X, input_size, length = get_sizes(X, dataset_size)\n",
    "print(X.shape)\n",
    "#Defining sizes for input/target data\n",
    "Y=np.zeros((dataset_size, batch_mod, output))\n",
    "#yhat=np.zeros((dataset_size, batch_mod, output))\n",
    "\n",
    "\n",
    "#***********************CALLING TARGET GENERATOR FUNCTION**********************\n",
    "file = 'database/chb01-summary.txt'\n",
    "Y = target_gen(output, batch_mod, dataset_size, batch_size, file)\n",
    "#******************************************************************************\n",
    "print(Y.shape)\n",
    "\n",
    "input_size_new = 23\n",
    "#elements of each frame\n",
    "batch_size_new = 100\n",
    "#time steps equivalent \n",
    "timesteps= 10 # meaning of this?\n",
    "#number of sequences\n",
    "seq_number = 100\n",
    "#Re-defining dataset size for training\n",
    "dataset_size = 16\n",
    "max_iters = 40 #nb_epochs\n",
    "#Defining sizes for input/target data\n",
    "X_new=np.zeros((seq_number*dataset_size, timesteps, input_size_new, batch_size_new, 1))\n",
    "#yhat=np.zeros((dataset_size, batch_mod, output))\n",
    "Y_new=np.zeros((seq_number*dataset_size, output))\n",
    "\n",
    "for m in range(0,dataset_size):\n",
    "    if (m == 3-1 or  m == 4-1 or m == 15-1 or m == 16-1 or m == 18-1 ):\n",
    "        if m == 3-1:\n",
    "            initial_time = 700000\n",
    "        if m == 4-1:\n",
    "            initial_time = 300000\n",
    "        if m == 15-1:\n",
    "            initial_time = 400000\n",
    "        if m == 16-1:\n",
    "            initial_time = 200000\n",
    "        if m == 18-1:\n",
    "            initial_time = 400000\n",
    "    else:\n",
    "        initial_time = 0\n",
    "    print('initial time:', initial_time)\n",
    "    for i in range(0,seq_number):\n",
    "        for j in range(0,timesteps):\n",
    "            initial = initial_time+(i*batch_size_new*timesteps)+j*batch_size_new\n",
    "            final = initial_time+(i*batch_size_new*timesteps)+((j+1)*batch_size_new)\n",
    "            #print(initial_time+(i*batch_size_new*timesteps)+j*batch_size_new)\n",
    "            #print(initial_time+(i*batch_size_new*timesteps)+((j+1)*batch_size_new))\n",
    "            X_new[seq_number*m+i,j,:,0:batch_size_new,0] =  X[m,0,0,0:input_size_new,initial:final,0] \n",
    "\n",
    "#standardizing data\n",
    "max_value = np.amax(abs(X_new))\n",
    "min_value = np.amin(abs(X_new))\n",
    "# print('MAX VALUE', max_value)\n",
    "X_new = X_new/max_value # standardization by max value (all values between 0 and 1)\n",
    "#size of data: (800, 10, 23, 100, 1)\n",
    "Y_new[seq_number*2+33*2:seq_number*2+39*2,output-1] = 1\n",
    "Y_new[seq_number*3+37*2:seq_number*3+42*2,output-1] = 1\n",
    "Y_new[seq_number*14+21*2:seq_number*14+27*2,output-1] = 1\n",
    "Y_new[seq_number*15+29*2:seq_number*15+37*2,output-1] = 1\n",
    "\n",
    "\n",
    "print(X_new.shape, Y_new.shape)\n",
    "\n",
    "# sequences, timesteps, features, batches. (100 x nb_files, 10,23, 100,,1)\n",
    "# define model\n",
    "# initialize all parameters\n",
    "sgd_batch_size = 1600\n",
    "layer_0 = Conv2D(2,2,2)\n",
    "# add activation\n",
    "layer_1 = MaxPool2D()\n",
    "#layer_2 = GRU(dataset_size*seq_number,timesteps,1078,100) #sequences, timesteps, features, outputs\n",
    "layer_2 = GRU(sgd_batch_size, timesteps, 1078, 100)\n",
    "layer_3 = Linear(100,1)\n",
    "X_GRU_flat_augmented = np.ones((sgd_batch_size,10,1079))\n",
    "#Lin_bias_col = np.ones((800)) # create column of 1's for the bias.\n",
    "S_GRU_augmented = np.ones((sgd_batch_size,101))\n",
    "# add activation\n",
    "#training phase\n",
    "\n",
    "#forward pass\n",
    "for n_iters in range(20):\n",
    "    #pick sequence randomly:\n",
    "    i = randint(0,dataset_size*seq_number/sgd_batch_size-1)\n",
    "    #conv layer\n",
    "    HConv = layer_0.forward(X_new[i*sgd_batch_size:(i+1)*sgd_batch_size,:,:,:,:]) # 5D data for train_data, 3D for Wconv 2D for Bconv\n",
    "    #print(HConv.shape)\n",
    "    YConv = reLU(HConv, deriv=False) # no requirement on shape\n",
    "    #pooling layer\n",
    "    pool_kernel = np.array([2,2])\n",
    "    YPool, XArgmax = layer_1.forward(YConv,  pool_kernel) #5D data for YConv\n",
    "    #print(\"YPOOOL\", YPool.shape)\n",
    "    #flattening\n",
    "    X_GRU_flat = YPool.reshape(YPool.shape[0],10,-1) # check size here should be 3D (100*nb_files, 10, 1078)\n",
    "    #print(X_GRU_flat.shape)\n",
    "    X_GRU_flat_augmented[:,:,1:1079]= X_GRU_flat\n",
    "    #GRU\n",
    "    S_GRU = layer_2.forward(X_GRU_flat_augmented)\n",
    "    last = S_GRU.shape[1]-1 # timesteps\n",
    "    S_GRU_augmented[:,1:101] =  S_GRU[:,last,:]\n",
    "    HLinear = layer_3.forward(S_GRU_augmented)\n",
    "    yhat = sigmoid(HLinear, deriv=False)\n",
    "    #print(yhat)\n",
    "    #calculate loss\n",
    "    loss = CrossEntropy(yhat, Y_new[i*sgd_batch_size:(i+1)*sgd_batch_size]) # works only for y = 0 or 1\n",
    "    print(loss)\n",
    "    #backward pass\n",
    "    #NOT NEEDED: dloss = derivCrossEntropy(yhat, y_train)\n",
    "    dhlin = np.zeros((yhat.shape))\n",
    "    #linear layer\n",
    "    #because binary cross-entropy is associated to sigmoid we have:\n",
    "    # dloss/dw = dloss/dyhat*dyhat/dwx*dwx/dw = (-y+yhat)*w\n",
    "    # dloss/dx = (-y + yhat)*x\n",
    "    # no need to explicitly compute dloss/dyhat, dyhat/dwx\n",
    "    #print(yhat.shape, Y_new.shape)\n",
    "    print(i, yhat.shape,Y_new.shape)\n",
    "    for k in range(yhat.shape[0]):\n",
    "        if Y_new[i*sgd_batch_size+k,:]==0:\n",
    "            dhlin[k,:] = 2*(yhat[k,:] - Y_new[i*sgd_batch_size+k,:])\n",
    "        else:\n",
    "            dhlin[k,:] = 20*(yhat[k,:] - Y_new[i*sgd_batch_size+k,:])\n",
    "    #only 0.5/-0.5\n",
    "    dxlin, dwlin = layer_3.backward(dhlin, S_GRU_augmented)\n",
    "\n",
    "    dxlin = np.delete(dxlin, 0, 1)\n",
    "    dsGRU, dxGRU = layer_2.backward(dxlin, X_GRU_flat_augmented)\n",
    "    #print(dxGRU.shape)\n",
    "    dxGRU = np.delete(dxGRU, 0, 2)\n",
    "    dyMaxPool = dxGRU.reshape(dxGRU.shape[0], dxGRU.shape[1],11,49,2)\n",
    "    #print(dyMaxPool.shape)\n",
    "    dxMaxPool = layer_1.backward(XArgmax, dyMaxPool) \n",
    "    #last column of convolution image\n",
    "    dxMaxPool_augmented = np.zeros((dxMaxPool.shape[0], dxMaxPool.shape[1], dxMaxPool.shape[2], dxMaxPool.shape[3]+1, dxMaxPool.shape[4]))\n",
    "    dxMaxPool_augmented[:,:,:,0:dxMaxPool.shape[3],:]=dxMaxPool\n",
    "    dhConv2D = reLU(dxMaxPool_augmented, deriv=True)\n",
    "    #print(dhConv2D.shape)\n",
    "    layer_0.backward(dhConv2D, X_new[i*sgd_batch_size:(i+1)*sgd_batch_size,:,:,:,:])\n",
    "\n",
    "    #wz, dwz,_,_,_,_,_,_,_,_,_,_ = layer_2.get_parameters()\n",
    "    #print(wz[580:585,0:10], dwz[580:585,0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error(yhat,y):\n",
    "    err = 0\n",
    "    yhat = np.round(yhat)\n",
    "    diff = yhat-y\n",
    "    for i in range(diff.shape[0]):\n",
    "        if diff != 0:\n",
    "            err+=1\n",
    "    err = err/diff.shape[0]\n",
    "    return err\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d Xshape (100, 10, 23, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#*******************************test phase***********************************************\n",
    "file = 18\n",
    "\n",
    "#Defining sizes for input/target data\n",
    "X_test=np.zeros((seq_number, timesteps, input_size_new, batch_size_new, 1))\n",
    "\n",
    "layer_2.change_input_size(100,10,100)\n",
    "X_GRU_flat_augmented_test = np.ones((100,10,1079))\n",
    "S_GRU_augmented_test = np.ones((100,101))\n",
    "initial_time = 400000\n",
    "\n",
    "for i in range(0,seq_number):\n",
    "    for j in range(0,timesteps):\n",
    "        #print(initial, final, batch_size_new, file-1)\n",
    "        initial = initial_time+(i*batch_size_new*timesteps)+j*batch_size_new\n",
    "        final = initial_time+(i*batch_size_new*timesteps)+((j+1)*batch_size_new)\n",
    "        #print(initial_time+(i*batch_size_new*timesteps)+j*batch_size_new)\n",
    "        #print(initial_time+(i*batch_size_new*timesteps)+((j+1)*batch_size_new))\n",
    "        X_test[i,j,:,0:batch_size_new,0] =  X[file-1,0,0,0:input_size_new,initial:final,0] \n",
    "   \n",
    "\n",
    "   \n",
    "max_value_2 = np.amax(abs(X_test))\n",
    "min_value = np.amin(abs(X_test))\n",
    "# print('MAX VALUE', max_value)\n",
    "X_test = X_test/max_value_2\n",
    "\n",
    "#generate predictions:\n",
    "HConv = layer_0.forward(X_test) # 5D data for train_data, 3D for Wconv 2D for Bconv\n",
    "YConv = reLU(HConv, deriv=False) # no requirement on shape\n",
    "#pooling layer\n",
    "pool_kernel = np.array([2,2])\n",
    "YPool, XArgmax = layer_1.forward(YConv,  pool_kernel) #5D data for YConv\n",
    "#flattening\n",
    "X_GRU_flat = YPool.reshape(YPool.shape[0],10,-1) # check size here should be 3D (100*nb_files, 10, 1078)\n",
    "#print(X_GRU_flat.shape)\n",
    "X_GRU_flat_augmented_test[:,:,1:1079]=X_GRU_flat\n",
    "#GRU\n",
    "S_GRU = layer_2.forward(X_GRU_flat_augmented_test)\n",
    "last = S_GRU.shape[1]-1 # timesteps\n",
    "S_GRU_augmented_test[:,1:101] =  S_GRU[:,last,:]\n",
    "HLinear = layer_3.forward(S_GRU_augmented_test)\n",
    "yhat_test = sigmoid(HLinear, deriv=False)\n",
    "file2 = open(\"CNN_GRU_testsetfile18_lowlevel\", 'w')\n",
    "np.savetxt(file2, yhat_test, delimiter=\",\" )\n",
    "file2.close()\n",
    "#test_err = compute_error(yhat_test, Y_test)\n",
    "#print(test_err)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 1.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round([1.5,1.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1,2],[3,4]])\n",
    "I = np.tile(A, (2,2,2))\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.01349721 -0.79802286]\n",
      "  [-0.01274448 -1.14339606]]\n",
      "\n",
      " [[ 0.00696058 -0.66652883]\n",
      "  [ 0.01983425 -0.17549277]]]\n",
      "[[-0.29894161 -1.45337415]]\n"
     ]
    }
   ],
   "source": [
    "print(layer_0.W)\n",
    "print(layer_0.B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.28619615]\n",
      " [ 0.67600959]\n",
      " [-0.299434  ]\n",
      " [ 0.28857047]\n",
      " [-0.69584952]\n",
      " [-0.04184838]\n",
      " [-0.95888501]\n",
      " [-0.76966287]\n",
      " [ 0.75901764]\n",
      " [-0.56461196]\n",
      " [-0.96454562]\n",
      " [ 0.02387505]\n",
      " [ 0.92176659]\n",
      " [-0.01527161]\n",
      " [-0.94178601]\n",
      " [ 0.49579442]\n",
      " [ 0.30305533]\n",
      " [-0.98547802]\n",
      " [ 0.54944936]\n",
      " [ 0.45959234]\n",
      " [ 0.86060038]\n",
      " [ 0.05158444]\n",
      " [-0.71606789]\n",
      " [-0.86182729]\n",
      " [-0.14422646]\n",
      " [ 0.76364925]\n",
      " [-0.41889778]\n",
      " [-0.63164139]\n",
      " [-0.97080978]\n",
      " [ 0.57151749]\n",
      " [ 0.98358668]\n",
      " [-0.51541532]\n",
      " [-0.17859179]\n",
      " [-0.35030104]\n",
      " [ 0.01420336]\n",
      " [ 0.3292313 ]\n",
      " [ 0.46236014]\n",
      " [ 0.66655303]\n",
      " [ 0.40879586]\n",
      " [ 0.63263982]\n",
      " [-0.27322773]\n",
      " [ 0.1328386 ]\n",
      " [-0.49677096]\n",
      " [-0.42392539]\n",
      " [-0.91191459]\n",
      " [-0.24041867]\n",
      " [-0.96867901]\n",
      " [ 0.35115342]\n",
      " [ 0.57394092]\n",
      " [ 0.62597609]\n",
      " [ 0.84378306]\n",
      " [ 0.05285628]\n",
      " [-0.99696805]\n",
      " [ 0.042918  ]\n",
      " [-0.23707146]\n",
      " [ 0.07739612]\n",
      " [ 0.05690764]\n",
      " [-0.20282921]\n",
      " [ 0.64222785]\n",
      " [ 0.19241508]\n",
      " [-0.25928168]\n",
      " [ 0.93267523]\n",
      " [ 0.20632538]\n",
      " [ 0.40934429]\n",
      " [-0.7589915 ]\n",
      " [-0.77674195]\n",
      " [-0.0120357 ]\n",
      " [ 0.56164864]\n",
      " [ 0.94352931]\n",
      " [ 0.3808357 ]\n",
      " [-0.98802616]\n",
      " [ 0.5815167 ]\n",
      " [ 0.18596408]\n",
      " [ 0.73681633]\n",
      " [ 0.26806151]\n",
      " [-0.19710553]\n",
      " [ 0.9937353 ]\n",
      " [ 0.09655235]\n",
      " [-0.72132562]\n",
      " [-0.26793784]\n",
      " [-0.89999709]\n",
      " [-0.64039711]\n",
      " [ 0.2030375 ]\n",
      " [-0.60086157]\n",
      " [-0.18144164]\n",
      " [ 0.36754582]\n",
      " [-0.82799309]\n",
      " [-0.4555277 ]\n",
      " [ 0.7752089 ]\n",
      " [ 0.64035431]\n",
      " [ 0.8352599 ]\n",
      " [-0.69628116]\n",
      " [-0.15358645]\n",
      " [-0.6520579 ]\n",
      " [-0.77381287]\n",
      " [-0.22817312]\n",
      " [ 0.79505793]\n",
      " [-0.07141452]\n",
      " [ 0.780816  ]\n",
      " [ 0.90226719]\n",
      " [ 0.96023037]]\n"
     ]
    }
   ],
   "source": [
    "print(layer_3.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03125 (1600, 1)\n"
     ]
    }
   ],
   "source": [
    "nb_ones =0\n",
    "for i in range(Y_new.shape[0]):\n",
    "    if Y_new[i]==1:\n",
    "        nb_ones +=1\n",
    "nb_ones/=1600\n",
    "print(nb_ones, Y_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
